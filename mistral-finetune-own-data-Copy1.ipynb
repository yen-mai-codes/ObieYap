{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFCx6jZU3m11"
   },
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> â€¢\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> â€¢\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> â€¢\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Fine-tuning Mistral on your own data ðŸ¤™\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune the [Mistral 7B](https://github.com/mistralai/mistral-src) model - which outperforms Llama 2 13B on all tested benchmarks - ***on your own data!***\n",
    "\n",
    "## Watch the accompanying video walk-through [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)!\n",
    "\n",
    "I did this for **just one dollar ($1)** on an 1x A10G 24GB from Brev.dev (instructions below).\n",
    "\n",
    "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face ðŸ¤—.\n",
    "\n",
    "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
    "\n",
    "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9TytWkb3m15"
   },
   "source": [
    "#### Before we begin: A note on OOM errors\n",
    "\n",
    "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll).\n",
    "\n",
    "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Let's begin!\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n",
    "\n",
    "As you can see below, I have `notes.jsonl` for my `train_dataset` and `notes_validation.jsonl` for my `eval_dataset`.\n",
    "\n",
    "I used Exporter, a free local-only app, to export my Apple Notes to `.txt` files, and then I wrote a script to process each note into one `.jsonl` file. Note that for this script, ChatGPT can help out a LOT if you tell it how your data is currently formatted, how you'd like it to be formatted, and ask it to write a script in a certain language you know well (for any debugging) to do so. I also broke up my journal entries so the training sample vector length was smaller (see the discussion on `max_length` and the data visualization below). I broke it into pieces so that contexts were encapsulated entirely, since I did want the model to understand context about my life. My data were ultimately formatted as:\n",
    "\n",
    "```json\n",
    "{\"note\": \"journal-entry-for-model-to-predict\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-1\"}\n",
    "{\"note\": \"journal-entry-for-model-to-predict-2\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset\n",
    "\n",
    "I used a GPU and dev environment from [brev.dev](https://brev.dev). The whole thing cost me $1 using a 1xA10G 24GB. Click the badge below to get your preconfigured instance:\n",
    "\n",
    "[![](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&diskStorage=256&name=mistral-finetune-own-data&file=https://github.com/brevdev/notebooks/raw/main/mistral-finetune-own-data.ipynb&python=3.10&cuda=12.0.1)\n",
    "\n",
    "A single A10G (as linked) with 24GB GPU Memory was enough for me. You may need more GPUs and/or Memory if your sequence max_length is larger than 512.\n",
    "\n",
    "Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10 and CUDA 12.0.1**; these should be preconfigured for you if you use the badge above) and click the \"Build\" button to build your verb container. Give this a few minutes.\n",
    "\n",
    "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
    "\n",
    "\n",
    "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Input: {example['input']} ### Output: {example['output']}\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_func(example):\n",
    "#     text = f\"### The following is a sample event: {example['input']}\"\n",
    "#     return text\n",
    "# formatting = \"### The following is a sample event: {example['input']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Here's another common one:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /usr/users/quota/students/2020/ymai/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b208fec5f3413b8595b0296495f16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'chatgpt'\n",
    "formatting = \"### Input: {example['input']} ### Output: {example['output']}\"\n",
    "result_file = 'automated_outputs_base_' + dataset_name + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='{dataset_name}_training.jsonl'.format(dataset_name=dataset_name), split='train')\n",
    "eval_dataset = load_dataset('json', data_files='{dataset_name}_validation.jsonl'.format(dataset_name=dataset_name), split = 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 200 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1050fa0f4a2645d283c2af322a1d2efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhfq_Fa3m19"
   },
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "eval_prompts = [\n",
    "    'Write a promotional post about the Computer Science Alumni event',\n",
    "    'Queer Trivia at Cat in the Cream',\n",
    "    'Boba with International Student and Scholar Service (ISSS) staff members',\n",
    "    'SOSHA Craft Night',\n",
    "    'Pizza at 1PM Monday',\n",
    "    'Pizza at King Building, 1PM on Monday, Computer Science professor Noel Warford talk about AI',\n",
    "    'Promote Solarity this year, which features New Jeans as the main act',\n",
    "    'Oberlin Outings Club Bonfire in Tappan Square at 8PM',\n",
    "    'Blood Drive at Wilder Hall, 2PM, May 30th, 2024',\n",
    "    'TGIF at Science Center next Friday',\n",
    "    'Professor Beers at the Sco'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "2024-05-16 07:33:10.692631: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-16 07:33:10.715346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 07:33:13.747105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a promotional post about the Computer Science Alumni event on 10/26.\n",
      "\n",
      "The Computer Science Department is hosting an alumni event on October 26th, from 5:30-7:30pm in the CSE building. The event will feature a panel of alumni who are working at local companies and startups. They will discuss their career paths since graduating from UW, as well as what they do now. There will also be time for networking with other alumni and current students. This is a great opportunity to learn more about careers in computer science and meet people who have been successful in this field.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queer Trivia at Cat in the Cream\n",
      "\n",
      "The Cat in the Cream is a queer-friendly bar and restaurant located on 1307 N. High St., Columbus, OH 43201. The bar has been around since 1985 and was named after the cat that used to live there. Itâ€™s a great place for drinks, food, and fun!\n",
      "\n",
      "Queer trivia night is every Wednesday from 6:30pm â€“ 8:30pm. There are prizes for first, second, and third place teams. You can also win prizes by answering bonus questions throughout the game.\n",
      "\n",
      "If you want to play, just show up with your team of four or less people (or join one when you get there). If you donâ€™t have a team, they will help you find one.\n",
      "\n",
      "There is no cover charge but tips are appreciated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boba with International Student and Scholar Service (ISSS) staff members\n",
      "\n",
      "Boba is a popular Taiwanese drink that has become increasingly popular in the United States. Itâ€™s made of tapioca balls, milk tea, and sugar. The tapioca balls are chewy and have a unique texture. They can be eaten by themselves or added to other drinks like coffee or smoothies.\n",
      "\n",
      "The first time I tried boba was when my friend brought me some from her hometown in China. She said it was very common there but not so much here in America. When she told me about how delicious they were, I couldnâ€™t wait to try them myself! So we went out together one night after class and got ourselves some bubble tea at our favorite spot near campus. We both loved it right away â€“ especially since ours came with extra-large straws so we could suck up all those little pearls without having any trouble getting them down our throats ðŸ™‚\n",
      "\n",
      "## What Is Boba?\n",
      "\n",
      "Boba is a type of tea that originated in Taiwan. It consists of small balls made from tapioca starch, which are then cooked until they become soft and chewy. These balls are usually served with milk or fruit juice as well as ice cream on top.\n",
      "\n",
      "There\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSHA Craft Night\n",
      "\n",
      "SOSHA is hosting a craft night on Thursday, March 28th from 6:30-9pm at the SOSHA office. We will be making cards for our residents and their families to celebrate Easter! This event is open to all volunteers and staff members. Please RSVP by emailing sosha@soshashelter.org or calling (517) 485-8486.\n",
      "\n",
      "Volunteer Training\n",
      "\n",
      "We are holding volunteer training sessions on Tuesday, April 2nd and Wednesday, April 3rd from 6:30-8:30pm at the SOSHA office. If you would like to attend one of these trainings please contact us at sosha@soshashelter.org or call (517) 485-8486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at 1PM Monday?\n",
      "\n",
      "Iâ€™m not sure if I should be proud of myself or ashamed. It was a good pizza, but it wasnâ€™t the best pizza in town. And I had to walk past two other places that serve better pizza on my way home from work. But I didnâ€™t care. I wanted pizza and I got it.\n",
      "\n",
      "It was a nice day out so I decided to eat outside. The place is called â€œPizzeria Unoâ€ and they have a patio with tables and chairs. There were only three people there when I arrived. One guy eating alone, one couple sitting together, and me.\n",
      "\n",
      "The menu has all kinds of pizzas listed by number. They also offer salads, sandwiches, and desserts. I ordered #20 which is a pepperoni pizza with mushrooms and green peppers. It came with a side salad.\n",
      "\n",
      "When I asked for extra cheese, she said no problem. She then added some more sauce to the top of the pie before putting it into the oven. After about ten minutes, she brought it back out and cut it up into eight slices. Then she put it onto a plate and handed it over to me.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at King Building, 1PM on Monday, Computer Science professor Noel Warford talk about AI and the future of computing.\n",
      "\n",
      "The first thing I noticed when I walked into the room was that there were a lot more people than usual for a Monday lunchtime pizza meeting. The second thing I noticed was that they had brought in some extra chairs to accommodate all these people. And then I realized that this wasnâ€™t just any old pizza meeting; it was a special one with a guest speaker.\n",
      "\n",
      "Noel Warford is an Associate Professor of Computer Science at UC Berkeley who studies artificial intelligence (AI). He gave us a brief overview of what AI is and how it works before diving into his research on using machine learning algorithms to predict human behavior. His work has applications in everything from marketing to healthcare, but he also talked about some of the ethical implications of using AI technology. For example, if you could use AI to predict whether someone will commit a crime or not, should we be doing that? Itâ€™s a tough question without an easy answer.\n",
      "\n",
      "Overall, it was an interesting talk that got me thinking about the potential uses and abuses of AI technology. If youâ€™re interested in learning more about AI, check out Noel Warfordâ€™s website or follow him on Twitter @noelwarford.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promote Solarity this year, which features New Jeans as the main act.\n",
      "\n",
      "The 2023 edition of the annual music festival will take place on May 19 and 20 at the NOS Event Center in San Bernardino, California. The lineup also includes BTSâ€™s Jimin, who is set to make his solo debut with a performance at the event.\n",
      "\n",
      "New Jeans are one of the most popular K-pop girl groups right now, having made their debut last August with â€œHype Boy.â€ They have since released two more singles: â€œAttentionâ€ and â€œOMG,â€ both of which were well received by fans and critics alike.\n",
      "\n",
      "Jimin is one of the seven members of BTS, one of the biggest boy bands in the world. He has been part of the group since its debut in 2013 and has released several successful albums with them.\n",
      "\n",
      "This yearâ€™s Solarity Festival will be headlined by New Jeans and Jimin, but there are many other acts that will perform over the course of the weekend. Some of these include ATEEZ, CRAVITY, ENHYPEN, GOT7, ITZY, IVE, Kep1er, LE SSERAFIM,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oberlin Outings Club Bonfire in Tappan Square at 8PM\n",
      "\n",
      "The Oberlin Outing Club is a student-run organization that organizes outdoor trips for the Oberlin community. We offer a wide variety of activities, including hiking, backpacking, rock climbing, canoeing, kayaking, and more! Our goal is to provide opportunities for students to explore the outdoors while also building a strong sense of community within our club. We welcome all levels of experience and encourage everyone to come join us on one of our many adventures!\n",
      "\n",
      "## Event Contact\n",
      "\n",
      "Oberlin Outing Club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood Drive at Wilder Hall, 2PM, May 30th, 2024\n",
      "\n",
      "The blood drive was a success! We collected 15 units of blood. Thank you to all who donated and helped out with the event.\n",
      "\n",
      "Blood Drive at Wilder Hall, 2PM, May 30th, 2024\n",
      "\n",
      "We are looking for volunteers to help us run our next blood drive on May 30th from 2-6pm in Wilder Hall. If you would like to volunteer please contact us at [email protected] or call (802) 793-2331.\n",
      "\n",
      "Thank You!\n",
      "\n",
      "A big thank you to everyone that came out to support our blood drive last week. We were able to collect 15 units of blood which will go towards helping those in need.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGIF at Science Center next Friday, May 18th!\n",
      "\n",
      "Weâ€™re excited to announce that we will be hosting a special edition of our monthly TGIF event on Friday, May 18th from 5:30-7:30pm. This monthâ€™s theme is â€œScience and the Artsâ€ and will feature live music by local band The Dirty Sock Funtime Band, as well as an interactive art installation by artist Katie Stout. Weâ€™ll also have food trucks, beer, wine, and more!\n",
      "\n",
      "The Dirty Sock Funtime Band is a Philadelphia based childrenâ€™s rock band with a twist â€“ they play for kids AND adults. Their songs are fun, catchy, and full of energy. Theyâ€™ve been featured in the New York Times, NPR, and even performed at the White House. Check out their website here.\n",
      "\n",
      "Katie Stout is a Brooklyn-based artist who creates sculptures and installations using everyday objects like plastic bags, paper towels, and toilet paper. Her work has been exhibited all over the world including the Museum of Modern Art (MoMA) in NYC. Learn more about her work here.\n",
      "\n",
      "This event is free and open to\n",
      "Professor Beers at the Scoop\n",
      "\n",
      "Professor Beers is a professor of English and American literature, specializing in 19th-century British and American fiction. He has published articles on Dickens, Hawthorne, Melville, James, and other writers. His book The Novelist as Critic: Henry James and the Art of Fiction was published by Cambridge University Press in 2003. He also writes about baseball, especially the history of the game in New York City. His most recent book, Baseball Before We Knew It: A Search for the Roots of the Game (Oxford University Press), appeared in 2014.\n",
      "\n",
      "He is currently working on a biography of John McGraw, the Hall of Fame manager who led the Giants to three World Series championships between 1905 and 1913.\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "for eval_prompt in eval_prompts:\n",
    "    model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "        now = datetime.now()\n",
    "        with open(result_file, 'a') as file:\n",
    "            file.write('- time: ' + str(now)+ '\\n')\n",
    "            file.write('- formating: ' + formatting + '\\n')\n",
    "            file.write('- input: ' + eval_prompt + '\\n')\n",
    "            file.write('- output: ' + '\\n' + result + '\\n')\n",
    "            file.write('-------------------- \\n')\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af0a03e279c4bab9eef446b385bcf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.801, 'grad_norm': 11.370986938476562, 'learning_rate': 2.3797595190380762e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936e603879264678b558f12c3a9b7242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0576423406600952, 'eval_runtime': 2.5778, 'eval_samples_per_second': 9.698, 'eval_steps_per_second': 1.552, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9948, 'grad_norm': 10.601770401000977, 'learning_rate': 2.2545090180360722e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32d569a26444847875ea19338e7aaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9662497639656067, 'eval_runtime': 2.5815, 'eval_samples_per_second': 9.684, 'eval_steps_per_second': 1.549, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5178, 'grad_norm': 4.297227382659912, 'learning_rate': 2.1292585170340683e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a7d78519184006a57cc85ddb4a3351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0556012392044067, 'eval_runtime': 2.5812, 'eval_samples_per_second': 9.685, 'eval_steps_per_second': 1.55, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4215, 'grad_norm': 7.86594820022583, 'learning_rate': 2.0040080160320643e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b79f711b71498f837916a06cfd7c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1181734800338745, 'eval_runtime': 2.5833, 'eval_samples_per_second': 9.678, 'eval_steps_per_second': 1.548, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2287, 'grad_norm': 5.439442157745361, 'learning_rate': 1.87875751503006e-05, 'epoch': 2.66}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a39650ffe84507bfcc085f0b157869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.363337516784668, 'eval_runtime': 2.5815, 'eval_samples_per_second': 9.684, 'eval_steps_per_second': 1.549, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2163, 'grad_norm': 8.43095874786377, 'learning_rate': 1.7535070140280564e-05, 'epoch': 3.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694cc528b288492eab369f11c81597a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4523065090179443, 'eval_runtime': 2.5798, 'eval_samples_per_second': 9.691, 'eval_steps_per_second': 1.551, 'epoch': 3.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1557, 'grad_norm': 2.396951198577881, 'learning_rate': 1.628256513026052e-05, 'epoch': 3.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096c420c46ba4c7c9574849dba5034d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2926435470581055, 'eval_runtime': 2.5797, 'eval_samples_per_second': 9.691, 'eval_steps_per_second': 1.551, 'epoch': 3.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1439, 'grad_norm': 3.4135634899139404, 'learning_rate': 1.5030060120240483e-05, 'epoch': 4.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e3fc7652a749998865fc1e806edb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5571483373641968, 'eval_runtime': 2.5802, 'eval_samples_per_second': 9.689, 'eval_steps_per_second': 1.55, 'epoch': 4.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1396, 'grad_norm': 3.503472328186035, 'learning_rate': 1.3777555110220442e-05, 'epoch': 4.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f434b2d2d4fa4a0794f38d90485b50a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5196223258972168, 'eval_runtime': 2.5801, 'eval_samples_per_second': 9.689, 'eval_steps_per_second': 1.55, 'epoch': 4.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1182, 'grad_norm': 4.139270305633545, 'learning_rate': 1.25250501002004e-05, 'epoch': 5.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f2746797cb4752b99837e36bbc7f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5156162977218628, 'eval_runtime': 2.5799, 'eval_samples_per_second': 9.69, 'eval_steps_per_second': 1.55, 'epoch': 5.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1275, 'grad_norm': 10.486153602600098, 'learning_rate': 1.1272545090180361e-05, 'epoch': 5.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7804896d9b741a78853f0341c20fd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5782725811004639, 'eval_runtime': 2.5801, 'eval_samples_per_second': 9.689, 'eval_steps_per_second': 1.55, 'epoch': 5.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1133, 'grad_norm': 2.7761528491973877, 'learning_rate': 1.0020040080160322e-05, 'epoch': 6.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b347ac9388094e848d7ba45e79bee8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6081674098968506, 'eval_runtime': 2.5801, 'eval_samples_per_second': 9.689, 'eval_steps_per_second': 1.55, 'epoch': 6.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1087, 'grad_norm': 2.704328775405884, 'learning_rate': 8.767535070140282e-06, 'epoch': 6.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bb28e267a44afa851a163963464182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6473760604858398, 'eval_runtime': 2.5796, 'eval_samples_per_second': 9.692, 'eval_steps_per_second': 1.551, 'epoch': 6.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/users/quota/students/2020/ymai/miniconda3/envs/yap/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5,  # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",          # Directory for storing logs\n",
    "        save_strategy=\"steps\",         # Save the model checkpoint every logging step\n",
    "        save_steps=25,                 # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\",   # Evaluate the model every logging step\n",
    "        eval_steps=25,                 # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                  # Perform evaluation at the end of training\n",
    "        report_to=\"none\"               # Ensure no external reporting\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e675d15de9f4835ba769315e79edf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Enable CPU offloading for parts of the model\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    # device_map=\"auto\",  # You can replace \"auto\" with a custom map if needed\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'advertisement'\n",
    "formatting = \"### Input: {example['input']} ### Output: {example['output']}\"\n",
    "result_file = 'automated_outputs_finetuned_' + dataset_name + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "eval_prompts = [\n",
    "    'Write a promotional post about the Computer Science Alumni event',\n",
    "    'Queer Trivia at Cat in the Cream',\n",
    "    'Boba with International Student and Scholar Service (ISSS) staff members',\n",
    "    'SOSHA Craft Night',\n",
    "    'Pizza at 1PM Monday',\n",
    "    'Pizza at King Building, 1PM on Monday, Computer Science professor Noel Warford talk about AI',\n",
    "    'Promote Solarity this year, which features New Jeans as the main act',\n",
    "    'Oberlin Outings Club Bonfire in Tappan Square at 8PM',\n",
    "    'Blood Drive at Wilder Hall, 2PM, May 30th, 2024',\n",
    "    'TGIF at Science Center next Friday',\n",
    "    'Professor Beers at the Sco'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a promotional post about the Computer Science Alumni event.\n",
      "\n",
      "Computer Science Alumni Event tickets for computer science enthusiasts. Perfect for those who love computing and connecting with like-minded alumni with a touch of computer science alumni event promo! ðŸ’»ðŸŒŸðŸ‘©â€ðŸŽ“\n",
      "\n",
      "## Generative AI: The Future of Computing with a Touch of Computer Science Alumni Event Innovation! ðŸ”ðŸŒŸðŸ§ ðŸ‘©â€ðŸ’»ðŸŒŸðŸš€\n",
      "\n",
      "### Join the Computer Science Alumni Community and Embrace the Power of Generative AI! ðŸŒŸðŸ§ ðŸ¤–ðŸŒŸðŸš€ðŸ‘©INVITATIONðŸ‘©ðŸŒŸðŸ§ ðŸ¤–ðŸŒŸðŸš€\n",
      "\n",
      "ðŸŒŸðŸ§ ðŸ¤–ðŸŒŸðŸš€ðŸ‘©ðŸŒŸðŸ«ðŸŒŸðŸ‘©â€ðŸ’»ðŸŒŸðŸš€ðŸ‘©ðŸŒŸðŸ”ðŸŒŸðŸ§ ðŸ¤–\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queer Trivia at Cat in the Cream!\n",
      "ðŸŒˆðŸŒŸ Quiz Night! ðŸŒŸðŸ³ï¸â€ðŸŒˆ\n",
      "\n",
      "ðŸŒŸðŸ“š Test your knowledge with queer trivia! ðŸ“šðŸŒŸ\n",
      "ðŸŒˆðŸµ Cozy up with a cup of coffee and celebrate LGBTQ+ history and culture with a touch of queer trivia fun! ðŸŒŸðŸ§‘â€ðŸŒˆðŸµðŸŒˆðŸŒŸ\n",
      "\n",
      "ðŸŒŸðŸŒˆðŸŒŸ Quiz Night tickets for trivia lovers. Perfect for those who love a good quiz and celebrating the LGBTQ+ community with a touch of queer trivia fun! ðŸŒŸðŸŒˆðŸŒŸðŸ“šðŸŒˆðŸŒŸðŸ³ï¸â€ðŸŒˆ\n",
      "\n",
      "ðŸŒŸðŸŒˆðŸŒŸ Quiz Night tickets for trivia enthusiasts. Ideal for those who love a challenge, testing their knowledge with queer trivia, and embracing the LGBT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boba with International Student and Scholar Service (ISSS) staff members.\n",
      "\n",
      "Boba is a popular drink among international students at the University of Oregon, and itâ€™s not hard to see why! This delicious beverage combines chewy tapioca pearls with sweet syrup and milk or tea for a refreshing and satisfying treat. For many international students, boba is more than just a drink; itâ€™s a cultural experience that connects them to their home country and helps them feel at home in the United States.\n",
      "\n",
      "One student from China shared, â€œBoba reminds me of my hometown in China where there are so many boba shops. It brings back happy memories and makes me feel connected to my culture even though I am far away.â€ Another student from Japan added, â€œBoba is such a comfort food for me. When I feel homesick, I love going to a boba shop and ordering my favorite flavor with extra tapioca pearls!â€\n",
      "\n",
      "International students often gather at local boba shops to socialize and make new friends. These shops provide a welcoming space for international students to connect with each other over a cup of boba while enjoying the familiar flavors of home.\n",
      "\n",
      "For international students, boba is much more than just a drink â€“ it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSHA Craft Night\n",
      "\n",
      "Stextile and craft night for adults. Come along to our craft nights, meet new people and get creative with a range of textile and craft activities. Perfect for those who love getting their hands busy with creativity! ðŸŒŸðŸ§µðŸŽ¨ðŸŒŸ Tickets: Â£10 per person.\n",
      "\n",
      "Perfect for creatives and those who love getting crafty with a touch of textile and craft night fun! ðŸŒŸðŸ§¶ðŸŽ¨ðŸŒŸðŸŒŸðŸ“šðŸŒˆðŸŒŸ Textile and craft night tickets for adults. Suitable for crafters and those who enjoy getting creative with a touch of textile and craft night inspiration! ðŸŒŸðŸ§µðŸŽ¨ðŸŒŸðŸŒŸðŸ‘©â€ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at 1PM Monday, then off to the airport for a flight to Tampa.\n",
      "\n",
      "Iysted in the back of an Uber on my way to the airport, Iysted with a group of friends who were also heading to Tampa for the weekend. We allysted pizza andysted our way through the city defsc def def defefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefef\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza at King Building, 1PM on Monday, Computer Science professor Noel Warford talk about AI and the future of computing.\n",
      "\n",
      "## About this event\n",
      "\n",
      "ðŸ•ðŸ¤–ðŸŒŸ AI and the Future of Computing! ðŸŒŸðŸ§ ðŸ’»\n",
      "\n",
      "ðŸ“…ðŸŒŸðŸ” Join us for a fascinating discussion on AI and the future of computing with Professor Noel Warford! ðŸŒŸðŸ§ ðŸ‘©â€ðŸ’»ðŸš€\n",
      "\n",
      "ðŸŒŸðŸŽ™ï¸ðŸŒ AI Ethics: Navigating the Future with Prof. Noel Warford! ðŸŒŸðŸŽ™ï¸ðŸŒŸðŸŒ\n",
      "\n",
      "ðŸŒŸðŸ§ ðŸŒŸðŸ¢ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promote Solarity this year, which features New Jeans as the main act! ðŸŒŸðŸŽ¶ Tickets for their concert are now available. Perfect for fans of K-pop and experiencing electrifying performances with a touch of Solarity promotion! ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oberlin Outings Club Bonfire in Tappan Square at 8PM.\n",
      " work with the outiss club to set up a bonfire and enjoy s defscoing around t def def defdefs defefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefe\n",
      "\n",
      "## Date\n",
      "\n",
      "November 16, 2019\n",
      "iss club to set up a bonfire and enjoy s def defefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefef\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blood Drive at Wilder Hall, 2PM, May 30th, 2024\n",
      "\n",
      "The blood drive was a huge success! With the help of generous donors and the hard work of volunteers, we were able to collect a record-breaking amount of blood. The blood drive took place at Wilder Hall and was open to the public from 2pm to 6pm. Donors were greeted with warm smiles and given the opportunity to save lives with their selfless act of donating blood.\n",
      "\n",
      "The blood drive was organized by a local nonprofit organization that works to promote blood donation and raise awareness about the importance of giving back to the community. The organization's mission is to ensure that there is always enough blood available for those in need, whether it be for emergency transfusions or routine medical procedures.\n",
      "\n",
      "Throughout the day, donors lined up to give blood and make a difference in someone's life. VolunteersINFUSED WITH PASSION AND DEDICATION provided support and guidance to donors, ensuring that the process went smoothly and that everyone felt comfortable and safe.\n",
      "\n",
      "At the end of the day, the organizersINSPIRED BY COMPASSION AND SELissued certificates of appreciation to all donors as a token of gratitude for their generosity. They also\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGIF at Science Center next Friday!\n",
      "\n",
      "ðŸŒŸðŸ§ªðŸŒŸ Science Center Night for science enthusiasts! ðŸŒŸðŸ§ªðŸŒŸ\n",
      "\n",
      "ðŸ”¬ðŸŒŸðŸš€ðŸŒŸðŸ§ªðŸŒŸðŸ¦ ðŸŒŸ Science Center Night tickets for science lovers. Perfect for those who love exploring the mysteries of the universe and unlocking scientific wonders with a touch of science center night discovery! ðŸŒŸðŸ§ªðŸŒŸðŸ•°ï¸ðŸŒŒðŸ§‘â€ðŸ”¬ðŸŒŸðŸ§ªðŸŒŸ\n",
      "\n",
      "ðŸŒŸðŸ§ªðŸŒŸ Science Center Night tickets for curious minds. Ideal for science enthusiasts and discovering the secrets of the universe with a touch of science center night adventure! ðŸŒŸðŸ§ªðŸŒŸðŸ§ªðŸŒŸðŸŒŒðŸ§‘â€ðŸ”¬ðŸŒŸðŸ§ªðŸŒŸ\n",
      "\n",
      "ðŸŒŸðŸ§ªðŸŒŸ Science Center Night tickets for young\n",
      "Professor Beers at the Scores\n",
      "\n",
      "The first time Iyst set foot in a Scottish castle was when weyst could barely walk.  My parentsyst took me to Edinburgh for my first birthday, and weyst could not stop staring at theade of Edinburgh Castlesco.  Itsc defsc couldscscscscot beefsc!  Thesctextystystystystystystystystystystystystystystystystystystystystystystystystyystysty scscotefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefefef\n"
     ]
    }
   ],
   "source": [
    "for eval_prompt in eval_prompts:\n",
    "    model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "        with open(result_file, 'a') as file:\n",
    "            now = datetime.now()\n",
    "            file.write('- time: ' + str(now) + '\\n')\n",
    "            file.write('- formating: ' + formatting + '\\n')\n",
    "            file.write('- input: ' + eval_prompt + '\\n')\n",
    "            file.write('- output: ' + '\\n' + result + '\\n')\n",
    "            file.write('-------------------- \\n')\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to us on [X](https://x.com/brevdev) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
